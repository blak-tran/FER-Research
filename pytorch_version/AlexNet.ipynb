{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/anishjohnson/Face-Emotion-Recognition/blob/main/FER/Colab%20Notebook/Face_Emotion_Recognition_Anish_Johnson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URwdYfsiMltF"
   },
   "source": [
    "# **Face Emotion Recognition**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5RpZlMkPGG2"
   },
   "source": [
    "# **Objective**\n",
    "Our objective is to solve the above mentioned challenge by applying deep learning algorithms to live video data inorder to recognize the facial emotions and categorize them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AGaR26YPV--"
   },
   "source": [
    "# **Dataset used**\n",
    "We have utilized the [FER 2013](https://www.kaggle.com/datasets/msambare/fer2013) dataset provided on Kaggle.<br>\n",
    "The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image.<br>\n",
    "\n",
    "The task is to categorize each face based on the emotion shown in the facial expression into one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The training set consists of 28,709 examples and the public test set consists of 3,589 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVjAMoriVm5s",
    "outputId": "b5426b3a-00a5-4881-9dd9-d8e81b392719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 15 09:59:58 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   68C    P0    28W /  70W |   5093MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKaCCukeQGGv"
   },
   "source": [
    "### **Let's Begin:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2QTffm_tEDK"
   },
   "source": [
    "# **Data Exploration:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5py4G2_K3R3Y"
   },
   "source": [
    "**Lets start by importing the required libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OheLkRm1DJ3a",
    "outputId": "22743bb5-ed17-4dff-f68c-a211f4125678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Above libraries have been imported.\n"
     ]
    }
   ],
   "source": [
    "# Basic python libraries.\n",
    "import glob\n",
    "import os.path as osp\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import itertools\n",
    "# Get rid of warnings!\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import shutil\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "print('Above libraries have been imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wC_Fow-Zr67X",
    "outputId": "dc88df82-6c8b-46e0-a98a-4606c381afd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wcIetsRMr5fQ"
   },
   "outputs": [],
   "source": [
    "file_path = '/content/drive/MyDrive/Colab Notebooks/Dataset/FER-2013-Dataset.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azMIO50rsGAW",
    "outputId": "99985171-ef61-4dcd-c8e3-157aba32ab96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip file extracted successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the FER2013 dataset\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(file_path , 'r') as ZipFile:\n",
    "  ZipFile.extractall()\n",
    "  print('Zip file extracted successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKEJMNa45tfB"
   },
   "source": [
    "**Seperate the Training and Validation Data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Duo9uIT3PV6"
   },
   "source": [
    "**Have a look at our data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNCuMOZrGkcx"
   },
   "source": [
    "**Perform some Data Augmentation on train and validations sets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvy5uusVtMAh"
   },
   "source": [
    "# **Data Preprocessing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "NnZfATJ1GW5T"
   },
   "outputs": [],
   "source": [
    "class ImageTransform():\n",
    "    def __init__(self, resize, mean, std):\n",
    "        self.data_transform = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.RandomResizedCrop(resize, scale = (0.5, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5), (0.5))\n",
    "            ]),\n",
    "            \n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(resize),\n",
    "                transforms.CenterCrop(resize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5), (0.5))\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "    def __call__(self, img, phase='train'):\n",
    "        return self.data_transform[phase](img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "iNG7N1bOknGr"
   },
   "outputs": [],
   "source": [
    "resize = 48\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform = ImageTransform(resize, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ZGEzA4H0b2uE"
   },
   "outputs": [],
   "source": [
    "def make_datapath_list(phase):\n",
    "    rootpath = '/content/'\n",
    "    target_path = osp.join(rootpath + phase + '/**/*.jpg')\n",
    "    print(target_path)\n",
    "    \n",
    "    path_list = []\n",
    "    \n",
    "    for path in glob.glob(target_path):\n",
    "        path_list.append(path)\n",
    "        \n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfQSzCIDGWvq",
    "outputId": "6657f08d-0e37-4e7f-96db-b8babc073d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/train/**/*.jpg\n",
      "/content/test/**/*.jpg\n",
      "28709\n",
      "(tensor([[[-0.6863, -0.5294, -0.3961,  ..., -0.7961, -0.8039, -0.8118],\n",
      "         [-0.6627, -0.5059, -0.3647,  ..., -0.8118, -0.8118, -0.8039],\n",
      "         [-0.6078, -0.4745, -0.3255,  ..., -0.8196, -0.8118, -0.8039],\n",
      "         ...,\n",
      "         [-0.6235, -0.6392, -0.6627,  ..., -0.2471, -0.2941, -0.3333],\n",
      "         [-0.6000, -0.6157, -0.6471,  ..., -0.2000, -0.2784, -0.3098],\n",
      "         [-0.5843, -0.6078, -0.6314,  ..., -0.1843, -0.2784, -0.3098]]]), 4)\n",
      "7178\n",
      "(tensor([[[ 0.6706,  0.7255,  0.7333,  ...,  0.6784,  0.6784,  0.6784],\n",
      "         [ 0.6863,  0.7255,  0.7255,  ...,  0.6784,  0.6784,  0.6706],\n",
      "         [ 0.6784,  0.7412,  0.7176,  ...,  0.6863,  0.6784,  0.6706],\n",
      "         ...,\n",
      "         [ 0.7412,  0.7647,  0.7647,  ..., -0.1137, -0.2863, -0.6784],\n",
      "         [ 0.7725,  0.7569,  0.7333,  ..., -0.1059,  0.0118, -0.6157],\n",
      "         [ 0.7490,  0.7176,  0.7333,  ..., -0.4118, -0.6314, -0.9529]]]), 4)\n",
      "All data have been loaded!!\n"
     ]
    }
   ],
   "source": [
    "train_list = make_datapath_list('train')\n",
    "val_list = make_datapath_list('test')\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, file_list, transform=None, phase=\"train\"):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_list[idx]\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        img_transformed = self.transform(img, self.phase)\n",
    "        \n",
    "        label = img_path.split('/')[3]\n",
    "        \n",
    "        if label == \"angry\":\n",
    "            label = 0\n",
    "        elif label == \"disgust\":\n",
    "            label = 1\n",
    "        elif label == \"fear\":\n",
    "            label = 2\n",
    "        elif label == \"happy\":\n",
    "            label = 3\n",
    "        elif label == \"neutral\":\n",
    "            label = 4\n",
    "        elif label == \"sad\":\n",
    "            label = 5\n",
    "        elif label == \"surprise\":\n",
    "            label = 6\n",
    "        return img_transformed, label\n",
    "\n",
    "\n",
    "train_set = MyDataset(train_list, transform = ImageTransform(resize, mean, std), phase = 'train')\n",
    "val_set = MyDataset(val_list, transform = ImageTransform(resize, mean, std), phase = 'val')\n",
    "\n",
    "index = 0\n",
    "print(train_set.__len__())\n",
    "print(train_set.__getitem__(index))\n",
    "\n",
    "print(val_set.__len__())\n",
    "print(val_set.__getitem__(index))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size, shuffle = False)\n",
    "\n",
    "dataloader_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "\n",
    "batch_iterator = iter(dataloader_dict[\"train\"])\n",
    "\n",
    "batch = next(batch_iterator)\n",
    "\n",
    "\n",
    "inputs, labels = batch\n",
    "\n",
    "print(\"All data have been loaded!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNBzIZBDQbAm"
   },
   "source": [
    "**Now lets create our custom CNN model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOwMRBUFuHnp"
   },
   "source": [
    "# **Build CNN Model:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YK5yRfKO2r53"
   },
   "source": [
    "**Before we start building the neural network lets understand some of the terms that we will be using.**\n",
    "\n",
    "* **Model = sequential** : A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "\n",
    "* **Padding** = The padding parameter of the Keras Conv2D class can take one of two values: 'valid' or 'same'. Setting the value to “valid” parameter means that the input volume is not zero-padded and the spatial dimensions are allowed to reduce via the natural application of convolution.\n",
    "\n",
    "* **Activation** = relu :The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.\n",
    "\n",
    "* **Maxpooling** = Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map. The results are down sampled or pooled feature maps that highlight the most present feature in the patch, not the average presence of the feature in the case of average pooling.\n",
    "\n",
    "* **Batch normalization** = Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n",
    "\n",
    "* **Dropout** = Dropout is a technique used to prevent a model from overfitting. Dropout works by randomly setting the outgoing edges of hidden units (neurons that make up hidden layers) to 0 at each update of the training phase.\n",
    "\n",
    "* **Adam** = Adaptive Moment Estimation is an algorithm for optimization technique for gradient descent. The method is really efficient when working with large problem involving a lot of data or parameters. It requires less memory and is efficient. Intuitively, it is a combination of the ‘gradient descent with momentum’ algorithm and the ‘RMSP’ algorithm. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9E0RPBYPqW3d",
    "outputId": "ace8b353-1a7c-4b35-d4cf-4064eeaa72b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print(torchvision.models.alexnet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "dnxVTMKcGWjd"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGQzWw6Dd6F1"
   },
   "source": [
    "#Training model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lNxJTYXeR7_",
    "outputId": "3dbb48a0-7548-49dc-8a35-b5f670cfcb7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (maxpool): AdaptiveMaxPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=18432, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_classes = 7\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AlexNet(num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkgDCp9csotr",
    "outputId": "0db6fc23-eaf4-4863-846c-26d3be830aee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "save_every = 50\n",
    "# define your loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, cooldown=0, min_lr=0.001, eps=1e-08)\n",
    "# define your checkpoint callback function\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar', save_every=save_every):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'best_model.pth.tar')\n",
    "    if state['epoch'] % save_every == 0:\n",
    "        torch.save(state, f'checkpoint_epoch{state[\"epoch\"]}.pth.tar')\n",
    "\n",
    "# initialize variables for tracking training progress and best validation accuracy\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_val_acc = 0.0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# train the model\n",
    "for epoch in range(epochs):\n",
    "    start=time.time()\n",
    "    # train for one epoch\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # move inputs and labels to the same device as the model\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update training loss and accuracy\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_acc += (predicted == labels).sum().item()\n",
    "\n",
    "    # calculate average training loss and accuracy for the epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_acc / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # move inputs and labels to the same device as the model\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_acc += (predicted == labels).sum().item()\n",
    "\n",
    "    # calculate average validation loss and accuracy for the epoch\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = val_acc / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # update the learning rate scheduler based on validation loss\n",
    "    scheduler.step(-val_loss)\n",
    "\n",
    "    # save checkpoint and best model\n",
    "    is_best = val_acc > best_val_acc\n",
    "    best_val_acc = max(val_acc, best_val_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc\n",
    "    }, is_best)\n",
    "\n",
    "    # print training progress for the epoch\n",
    "    stop=time.time()\n",
    "    print(f'Time: {stop-start} Epoch {epoch+1} Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f} Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f} Best Val Acc: {best_val_acc:.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnmN7X0Lt88s"
   },
   "source": [
    "#Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1WdFN3cXW8Q"
   },
   "outputs": [],
   "source": [
    "model_dir = \"AlexNet_model.h5\"\n",
    "model_weights_dir = \"AlexNet_model_weighs.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQlkFuj_t8II"
   },
   "outputs": [],
   "source": [
    "# Save the whole model\n",
    "model.save(model_dir)\n",
    "model.save_weights(model_weights_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRQ3-GAkutyh"
   },
   "source": [
    "# **Model evaluation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNo3wp8ecPis"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "weights_path = \"/content/drive/MyDrive/Project-AI/model_saved/Fer_weights/AlexNetnew_model_weighs_200.h5\"\n",
    "\n",
    "def get_accuracy_from_weights(weights_path, val_set):\n",
    "    # Load model from weights\n",
    "    model = AlexNet((48,48,3),7) # replace create_model with your function to create the model architecture\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    # Evaluate model on test data\n",
    "    loss, accuracy = model.evaluate(val_set)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKE7KO6CdFdY"
   },
   "outputs": [],
   "source": [
    "accuracy = get_accuracy_from_weights(weights_path, val_set)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OirCn85XRRjJ"
   },
   "outputs": [],
   "source": [
    "# Or using plotting \n",
    "# Create plots for accuracy and loss.\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "fig.set_size_inches(12,4)\n",
    "\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('Training Accuracy vs Validation Accuracy')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('Training Loss vs Validation Loss')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_4f0pr_MvTf"
   },
   "outputs": [],
   "source": [
    "# Create confusion matrix of our predictions\n",
    "\n",
    "# compute predictions\n",
    "predictions = model.predict_generator(generator=val_set)\n",
    "y_pred = [np.argmax(probas) for probas in predictions]\n",
    "y_test = val_set.classes\n",
    "class_names = val_set.class_indices.keys()\n",
    "\n",
    "# Create function to plot confussion matrix.\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(12,6), dpi=120)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')# **Live Class Monitoring System(Face Emotion Recognition)**\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpwLo3MOVlo_"
   },
   "source": [
    "## Pretrain Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V69pX_lbVp0U"
   },
   "source": [
    "Load model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgloycjsVovY"
   },
   "source": [
    "loaded_model = load_model(\"AlexNet_model-100iters.h5\")\n",
    "loaded_model.load_weights(\"AlexNet_model_weights-100iters.h5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "URwdYfsiMltF",
    "g5RpZlMkPGG2"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
